# -*- coding: utf-8 -*-
"""TopKusingPGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1azbYhJAhIi1uD8k7xMXjUDUWkKEYVxNJ
"""

!pip install torch torch_geometric

# Re-import torch_geometric with required utilities
from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix

import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv
from torch_geometric.utils import degree

# Load Cora dataset
dataset = Planetoid(root="/tmp/Cora", name="Cora")
data = dataset[0].clone()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
data = data.to(device)

# Define GCN
class GCN(torch.nn.Module):
    def __init__(self, in_feats, hidden_feats, out_feats):
        super().__init__()
        self.conv1 = GCNConv(in_feats, hidden_feats)
        self.conv2 = GCNConv(hidden_feats, out_feats)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        return self.conv2(x, edge_index)

# Train GCN
def train(model, data, epochs=200):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
    model.train()
    for _ in range(epochs):
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()
    return model

# Select low-confidence nodes
def low_confidence_nodes(model, data, k=10):
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index)
        probs = F.softmax(out, dim=1)
        max_probs, _ = probs.max(dim=1)
    _, indices = torch.topk(-max_probs, k)
    return indices.long()

# PGD Attack with low-confidence nodes
def pgd_feature_attack(model, data, eps=0.5, alpha=0.05, iters=50, top_k=10):
    model.eval()
    x_adv = data.x.clone().detach().to(device)
    x_adv.requires_grad = True

    target_nodes = low_confidence_nodes(model, data, k=top_k)

    for _ in range(iters):
        out = model(x_adv, data.edge_index)
        loss = F.cross_entropy(out[target_nodes], data.y[target_nodes])
        loss.backward()

        with torch.no_grad():
            grad_sign = x_adv.grad.sign()
            x_adv[target_nodes] = x_adv[target_nodes] + alpha * grad_sign[target_nodes]
            perturb = torch.clamp(x_adv - data.x, min=-eps, max=eps)
            x_adv = (data.x + perturb).detach()
            x_adv.requires_grad = True

    return x_adv, target_nodes

# Evaluation
def evaluate_metrics(model, x, data, target_nodes):
    model.eval()
    out = model(x, data.edge_index)
    pred = out.argmax(dim=1)
    acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()
    loss = F.cross_entropy(out[data.test_mask], data.y[data.test_mask]).item()
    asr = (pred[target_nodes] != data.y[target_nodes]).float().mean().item()
    aml = (x != data.x).float().sum(dim=1).mean().item()
    return acc, loss, asr, aml

# Run
model = GCN(data.num_node_features, 16, dataset.num_classes).to(device)
model = train(model, data)

# Evaluate before attack
acc_before, loss_before, _, _ = evaluate_metrics(model, data.x, data, torch.tensor([], dtype=torch.long))

# Attack
x_attacked, target_nodes = pgd_feature_attack(model, data)

# Evaluate after attack
acc_after, loss_after, asr, aml = evaluate_metrics(model, x_attacked, data, target_nodes)

print(f"Accuracy before attack: {acc_before:.4f}")
print(f"Loss before attack:     {loss_before:.4f}")
print(f"Accuracy after attack:  {acc_after:.4f}")
print(f"Loss after attack:      {loss_after:.4f}")
print(f"Attack Success Rate (ASR): {asr * 100:.2f}%")
print(f"Average Modified Features (AML): {aml:.4f}")

# Run everything
model = GCN(data.num_node_features, 16, dataset.num_classes).to(device)
model = train(model, data)

# Evaluate before attack
model.eval()
out_clean = model(data.x, data.edge_index)
acc_clean = (out_clean[data.test_mask].argmax(dim=1) == data.y[data.test_mask]).float().mean().item()

def pgd_feature_attack(model, data, eps=0.05, alpha=0.01, iters=20, top_k=5):
    model.eval()
    x_adv = data.x.clone().detach().to(device)
    x_adv.requires_grad = True

    # Compute node degrees and get top-k
    deg = degree(data.edge_index[0])
    target_nodes = deg.topk(top_k).indices.long()

    for _ in range(iters):
        out = model(x_adv, data.edge_index)
        loss = F.cross_entropy(out[target_nodes], data.y[target_nodes])
        loss.backward()

        # PGD update
        with torch.no_grad():
            grad_sign = x_adv.grad.sign()
            x_adv[target_nodes] = x_adv[target_nodes] + alpha * grad_sign[target_nodes]
            perturb = torch.clamp(x_adv - data.x, min=-eps, max=eps)
            x_adv = (data.x + perturb).detach()
            x_adv.requires_grad = True

    return x_adv

# Attack and evaluate
x_attacked = pgd_feature_attack(model, data)
out_attacked = model(x_attacked, data.edge_index)
acc_attacked = (out_attacked[data.test_mask].argmax(dim=1) == data.y[data.test_mask]).float().mean().item()

# Print results
print(f"Accuracy before attack: {acc_clean:.4f}")
print(f"Accuracy after attack: {acc_attacked:.4f}")

# Attack and evaluate
x_attacked = pgd_feature_attack(model, data)
out_attacked = model(x_attacked, data.edge_index)
acc_attacked = (out_attacked[data.test_mask].argmax(dim=1) == data.y[data.test_mask]).float().mean().item()

# Evaluate before attack
model.eval()
with torch.no_grad():
    out = model(data.x, data.edge_index)
    acc_before = (out[data.test_mask].argmax(dim=1) == data.y[data.test_mask]).float().mean().item()
    loss_before = F.cross_entropy(out[data.test_mask], data.y[data.test_mask]).item()

from torch_geometric.utils import to_networkx, from_networkx
import networkx as nx
import random

# PGD attack on top-k degree nodes
def pgd_feature_attack(model, data, eps=0.05, alpha=0.01, iters=20, top_k=5):
    model.eval()
    x = data.x.clone().detach()
    x.requires_grad = True

    # Compute node degrees
    deg = degree(data.edge_index[0])
    topk_nodes = deg.topk(top_k).indices

    for _ in range(iters):
        out = model(x, data.edge_index)
        loss = F.cross_entropy(out[topk_nodes], data.y[topk_nodes])
        loss.backward()

        # Update features with PGD
        grad_sign = x.grad.sign()
        x.data[topk_nodes] = x.data[topk_nodes] + alpha * grad_sign[topk_nodes]
        perturb = torch.clamp(x.data - data.x, min=-eps, max=eps)
        x.data = data.x + perturb
        x.grad.zero_()

    return x

# Instantiate and train model
model = GCN(data.num_node_features, 16, dataset.num_classes).to(device)
model = train(model, data)

# Thực hiện flip edge attack
data_attacked, total_flips = edge_flip_attack(data, top_k=20, flips_per_node=3)

# Sau attack
data_attacked = data_attacked.to(data.x.device)
model.eval()
with torch.no_grad():
    out = model(data_attacked.x, data_attacked.edge_index)
    acc_after = (out[data.test_mask].argmax(dim=1) == data.y[data.test_mask]).float().mean().item()
    loss_after = F.cross_entropy(out[data.test_mask], data.y[data.test_mask]).item()

# Tính chỉ số
ASR = max(0, acc_before - acc_after)
AML = total_flips / 20  # 20 nodes bị tấn công

print(f"Before Attack - Accuracy: {acc_before:.4f}, Loss: {loss_before:.4f}")
print(f"After Attack  - Accuracy: {acc_after:.4f}, Loss: {loss_after:.4f}")
print(f"ASR (Attack Success Rate): {ASR:.4f}")
print(f"AML (Avg. Modified Links): {AML:.2f}")

# Apply PGD
x_attacked = pgd_feature_attack(data, model)
data_attacked = data.clone()
data_attacked.x = x_attacked

# Evaluate after attack
model.eval()
with torch.no_grad():
    out = model(data_attacked.x, data_attacked.edge_index)
    acc_after = (out[data.test_mask].argmax(dim=1) == data.y[data.test_mask]).float().mean().item()
    loss_after = F.cross_entropy(out[data.test_mask], data.y[data.test_mask]).item()

# Report
print(f"Before Attack - Accuracy: {acc_before:.4f}, Loss: {loss_before:.4f}")
print(f"After Attack  - Accuracy: {acc_after:.4f}, Loss: {loss_after:.4f}")
print(f"ASR (Attack Success Rate): {acc_before - acc_after:.4f}")
print(f"AML (Avg. Modified Links): 0.00")

# Attack Success Rate & AML (Avg Modified Links is not meaningful for feature attack)
ASR = max(0, before_acc - after_acc)
AML = 0  # not applicable here

print(f"Before Attack - Acc: {before_acc:.4f}, Loss: {before_loss:.4f}")
print(f"After Attack  - Acc: {after_acc:.4f}, Loss: {after_loss:.4f}")
print(f"ASR: {ASR:.4f}, AML: {AML}")